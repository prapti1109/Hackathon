{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11129847,"sourceType":"datasetVersion","datasetId":6941346}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom subprocess import check_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom pandas.plotting import lag_plot\nfrom datetime import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T18:57:22.310198Z","iopub.execute_input":"2025-03-22T18:57:22.310507Z","iopub.status.idle":"2025-03-22T18:57:24.504974Z","shell.execute_reply.started":"2025-03-22T18:57:22.310486Z","shell.execute_reply":"2025-03-22T18:57:24.504057Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/minute/nifty50minute.csv\")\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T18:57:24.506181Z","iopub.execute_input":"2025-03-22T18:57:24.506591Z","iopub.status.idle":"2025-03-22T18:57:25.792398Z","shell.execute_reply.started":"2025-03-22T18:57:24.506568Z","shell.execute_reply":"2025-03-22T18:57:25.791405Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data.shape\ndata.reset_index(drop=True, inplace=True)\ndata.drop(['volume'], axis=1, inplace=True)\ndata.drop(['Date'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T18:57:25.794068Z","iopub.execute_input":"2025-03-22T18:57:25.794418Z","iopub.status.idle":"2025-03-22T18:57:25.871804Z","shell.execute_reply.started":"2025-03-22T18:57:25.794384Z","shell.execute_reply":"2025-03-22T18:57:25.871130Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df = data.iloc[500000:,:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:27:32.433990Z","iopub.execute_input":"2025-03-22T19:27:32.434415Z","iopub.status.idle":"2025-03-22T19:27:32.439114Z","shell.execute_reply.started":"2025-03-22T19:27:32.434389Z","shell.execute_reply":"2025-03-22T19:27:32.438201Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Load OHLC data (ensure df has columns: 'open', 'high', 'low', 'close')\n\nscaler = MinMaxScaler()\ndata_scaled = scaler.fit_transform(df)  # Normalize data\n\n# Convert data into sequences\ndef create_sequences(data, seq_len):\n    sequences, targets = [], []\n    for i in range(len(data) - seq_len):\n        seq = data[i:i + seq_len]  # Input sequence (OHLC)\n        target = data[i + seq_len, 3]  # Predict 'close' price\n        sequences.append(seq)\n        targets.append(target)\n    return np.array(sequences), np.array(targets)\n\nseq_len = 30  # Sequence length\nX, y = create_sequences(data_scaled, seq_len)\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Convert to PyTorch tensors\nX_train_torch = torch.tensor(X_train, dtype=torch.float32)\ny_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n\nX_test_torch = torch.tensor(X_test, dtype=torch.float32)\ny_test_torch = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# Move tensors to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nX_train_torch, y_train_torch = X_train_torch.to(device), y_train_torch.to(device)\nX_test_torch, y_test_torch = X_test_torch.to(device), y_test_torch.to(device)\n\n# Print shapes\nprint(\"Train set:\", X_train_torch.shape, y_train_torch.shape)\nprint(\"Test set:\", X_test_torch.shape, y_test_torch.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:51:40.254669Z","iopub.execute_input":"2025-03-22T19:51:40.255096Z","iopub.status.idle":"2025-03-22T19:51:41.272226Z","shell.execute_reply.started":"2025-03-22T19:51:40.255062Z","shell.execute_reply":"2025-03-22T19:51:41.271424Z"}},"outputs":[{"name":"stdout","text":"Train set: torch.Size([346332, 30, 4]) torch.Size([346332, 1])\nTest set: torch.Size([86584, 30, 4]) torch.Size([86584, 1])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# ========================================\n# 1. Setup Device (GPU if available)\n# ========================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ========================================\n# 2. Define Self-Attention Module\n# ========================================\nclass SelfAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Linear(hidden_dim, hidden_dim)\n        self.key   = nn.Linear(hidden_dim, hidden_dim)\n        self.value = nn.Linear(hidden_dim, hidden_dim)\n        self.scale = torch.sqrt(torch.tensor(hidden_dim, dtype=torch.float))\n    \n    def forward(self, x):\n        # x shape: (batch_size, seq_len, hidden_dim)\n        Q = self.query(x)\n        K = self.key(x)\n        V = self.value(x)\n        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        return attn_output, attn_weights\n\n# ========================================\n# 3. Define the Agent Network (Per-Agent Model)\n# ========================================\nclass AgentNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, lstm_layers, seq_len):\n        \"\"\"\n        Args:\n            input_dim: Number of features (4 for OHLC).\n            hidden_dim: Dimension for LSTM and subsequent layers.\n            lstm_layers: Number of LSTM layers.\n            seq_len: Length of the input time-series window.\n        \"\"\"\n        super(AgentNetwork, self).__init__()\n        self.seq_len = seq_len\n        self.hidden_dim = hidden_dim\n        \n        # LSTM for time-series processing\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=lstm_layers, batch_first=True)\n        \n        # Self-Attention Layer to weigh important time steps\n        self.attention = SelfAttention(hidden_dim)\n        \n        # Fully connected layer to reduce the flattened sequence\n        self.fc = nn.Linear(seq_len * hidden_dim, hidden_dim)\n        \n        # Output regression head (predicts a single price value)\n        self.out = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, x):\n        # x: (batch_size, seq_len, input_dim)\n        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, hidden_dim)\n        attn_out, _ = self.attention(lstm_out)  # (batch_size, seq_len, hidden_dim)\n        \n        # Flatten the attention output\n        flat = attn_out.contiguous().view(x.size(0), -1)  # (batch_size, seq_len*hidden_dim)\n        features = F.relu(self.fc(flat))  # (batch_size, hidden_dim)\n        \n        # Regression output (price prediction)\n        prediction = self.out(features)  # (batch_size, 1)\n        return prediction, features\n\n# ========================================\n# 4. Define the Fusion Layer (Meta-Learner)\n# ========================================\nclass FusionLayer(nn.Module):\n    def __init__(self, num_agents, agent_feature_dim, fusion_hidden_dim):\n        \"\"\"\n        Args:\n            num_agents: Number of agent models.\n            agent_feature_dim: Dimension of the feature vector output from each agent.\n            fusion_hidden_dim: Hidden dimension for the fusion layer.\n        \"\"\"\n        super(FusionLayer, self).__init__()\n        self.fc1 = nn.Linear(num_agents * agent_feature_dim, fusion_hidden_dim)\n        self.fc2 = nn.Linear(fusion_hidden_dim, 1)\n        \n    def forward(self, features_list):\n        # Concatenate features from all agents\n        fusion_input = torch.cat(features_list, dim=1)  # (batch_size, num_agents*agent_feature_dim)\n        x = F.relu(self.fc1(fusion_input))\n        output = self.fc2(x)  # Final predicted price\n        return output\n\n# ========================================\n# 5. Define the Multi-Agent Ensemble Model\n# ========================================\nclass MultiAgentEnsemble(nn.Module):\n    def __init__(self, input_dim, hidden_dim, lstm_layers, seq_len, num_agents, fusion_hidden_dim):\n        \"\"\"\n        Args:\n            input_dim: Number of features per time step (4 for OHLC).\n            hidden_dim: Hidden dimension used by each agent.\n            lstm_layers: Number of LSTM layers in each agent.\n            seq_len: Length of the input sequence.\n            num_agents: Number of agents in the ensemble.\n            fusion_hidden_dim: Hidden dimension for the fusion layer.\n        \"\"\"\n        super(MultiAgentEnsemble, self).__init__()\n        self.num_agents = num_agents\n        \n        # Create a ModuleList of agents\n        self.agents = nn.ModuleList([\n            AgentNetwork(input_dim, hidden_dim, lstm_layers, seq_len) for _ in range(num_agents)\n        ])\n        \n        # Fusion layer that combines agent features to output final prediction\n        self.fusion = FusionLayer(num_agents, hidden_dim, fusion_hidden_dim)\n        \n    def forward(self, x):\n        # x: (batch_size, seq_len, input_dim)\n        agent_predictions = []\n        agent_features = []\n        for agent in self.agents:\n            pred, feat = agent(x)\n            agent_predictions.append(pred)\n            agent_features.append(feat)\n        \n        # Final output from fusion layer (meta-learner)\n        fusion_output = self.fusion(agent_features)\n        return agent_predictions, fusion_output\n\n# ========================================\n# 6. Data Preparation Functions\n# ========================================\ndef create_sequences(data, seq_length, prediction_horizon=5):\n    \"\"\"\n    Create sequences of data with a target value that's prediction_horizon steps ahead.\n    \n    Args:\n        data: Normalized numpy array of OHLC data\n        seq_length: Length of input sequence\n        prediction_horizon: How many steps ahead to predict (default: 5 minutes ahead)\n    \n    Returns:\n        X: Input sequences\n        y: Target values (close prices prediction_horizon steps ahead)\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length - prediction_horizon):\n        # Sequence of seq_length datapoints\n        seq = data[i:i+seq_length]\n        # Target is the close price prediction_horizon steps ahead\n        target = data[i+seq_length+prediction_horizon-1, 3]  # Index 3 is Close price\n        X.append(seq)\n        y.append(target)\n    return np.array(X), np.array(y).reshape(-1, 1)\n\ndef prepare_data(df, seq_length=30, prediction_horizon=5, train_split=0.8, batch_size=32):\n    \"\"\"\n    Prepare data for training and testing.\n    \n    Args:\n        df: Pandas DataFrame with OHLC data\n        seq_length: Length of input sequences\n        prediction_horizon: How many steps ahead to predict\n        train_split: Fraction of data to use for training\n        batch_size: Batch size for DataLoader\n    \n    Returns:\n        train_loader: DataLoader for training data\n        test_loader: DataLoader for testing data\n        scaler: Fitted scaler for inverse transformation\n    \"\"\"\n    # Ensure we have OHLC columns\n    if not all(col in df.columns for col in ['open', 'high', 'low', 'close']):\n        raise ValueError(\"DataFrame must contain 'open', 'high', 'low', 'close' columns\")\n    \n    # Extract OHLC data\n    data = df[['open', 'high', 'low', 'close']].values\n    \n    # Normalize data\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data_normalized = scaler.fit_transform(data)\n    \n    # Create sequences\n    X, y = create_sequences(data_normalized, seq_length, prediction_horizon)\n    \n    # Train-test split\n    train_size = int(len(X) * train_split)\n    X_train, X_test = X[:train_size], X[train_size:]\n    y_train, y_test = y[:train_size], y[train_size:]\n    \n    # Convert to PyTorch tensors\n    X_train_tensor = torch.FloatTensor(X_train)\n    y_train_tensor = torch.FloatTensor(y_train)\n    X_test_tensor = torch.FloatTensor(X_test)\n    y_test_tensor = torch.FloatTensor(y_test)\n    \n    # Create datasets and dataloaders\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, test_loader, scaler\n\n# ========================================\n# 7. Training Functions\n# ========================================\ndef train_model(model, train_loader, test_loader, epochs=50, lr=0.001):\n    \"\"\"\n    Train the model.\n    \n    Args:\n        model: The MultiAgentEnsemble model\n        train_loader: DataLoader for training data\n        test_loader: DataLoader for testing data\n        epochs: Number of training epochs\n        lr: Learning rate\n    \n    Returns:\n        model: Trained model\n        losses: List of training losses\n    \"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n    \n    model.to(device)\n    losses = []\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            # Forward pass\n            agent_preds, fusion_pred = model(X_batch)\n            \n            # Loss on fusion prediction\n            loss = criterion(fusion_pred, y_batch)\n            \n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_val, y_val in test_loader:\n                X_val, y_val = X_val.to(device), y_val.to(device)\n                _, fusion_pred = model(X_val)\n                val_loss += criterion(fusion_pred, y_val).item()\n        \n        avg_train_loss = epoch_loss / len(train_loader)\n        avg_val_loss = val_loss / len(test_loader)\n        losses.append(avg_train_loss)\n        \n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n        \n        # Learning rate scheduling\n        scheduler.step(avg_val_loss)\n    \n    return model, losses\n\n# ========================================\n# 8. Example Usage with Real Data\n# ========================================\nif __name__ == \"__main__\":\n    # Assume df is your DataFrame with OHLC data at 1-minute intervals\n    # df should have columns: 'open', 'high', 'low', 'close'\n    \n    # Example of loading data (uncomment and modify as needed)\n    # df = pd.read_csv('your_ohlc_data.csv')\n    \n    # For demonstration, we'll create a dummy dataframe if the actual one isn't available\n    # In your actual code, use your real dataframe instead of this dummy one\n    try:\n        # This line will throw a NameError if 'df' is not defined\n        print(f\"Using existing dataframe with shape: {df.shape}\")\n    except NameError:\n        print(\"Creating dummy dataframe for demonstration. Replace with your actual dataframe.\")\n        dates = pd.date_range(start='2023-01-01', periods=10000, freq='1min')\n        df = pd.DataFrame({\n            'datetime': dates,\n            'open': np.random.normal(100, 5, 10000),\n            'high': np.random.normal(105, 5, 10000),\n            'low': np.random.normal(95, 5, 10000),\n            'close': np.random.normal(100, 5, 10000)\n        })\n        # Make sure high is higher than open and close, and low is lower\n        for i in range(len(df)):\n            values = [df.loc[i, 'open'], df.loc[i, 'close']]\n            df.loc[i, 'high'] = max(values) + abs(np.random.normal(0, 1))\n            df.loc[i, 'low'] = min(values) - abs(np.random.normal(0, 1))\n    \n    # Hyperparameters\n    input_dim = 4          # OHLC (4 features)\n    hidden_dim = 64        # LSTM hidden size\n    lstm_layers = 1        # Number of LSTM layers\n    seq_len = 30           # Input window: last 30 minutes of data\n    num_agents = 3         # Number of agents in the ensemble\n    fusion_hidden_dim = 128  # Fusion layer hidden dimension\n    batch_size = 32\n    epochs = 50\n    prediction_horizon = 5  # Predict price 5 minutes ahead\n    \n    # Prepare data\n    train_loader, test_loader, scaler = prepare_data(\n        df=df, \n        seq_length=seq_len,\n        prediction_horizon=prediction_horizon,\n        batch_size=batch_size\n    )\n    \n    # Initialize model\n    model = MultiAgentEnsemble(\n        input_dim=input_dim,\n        hidden_dim=hidden_dim,\n        lstm_layers=lstm_layers,\n        seq_len=seq_len,\n        num_agents=num_agents,\n        fusion_hidden_dim=fusion_hidden_dim\n    )\n    \n    # Train model\n    trained_model, losses = train_model(\n        model=model,\n        train_loader=train_loader,\n        test_loader=test_loader,\n        epochs=epochs\n    )\n    \n    # Save the trained model\n    torch.save(trained_model.state_dict(), 'multi_agent_trading_model.pth')\n    print(\"Model saved to 'multi_agent_trading_model.pth'\")\n    \n    # Example of making predictions with the trained model\n    model.eval()\n    with torch.no_grad():\n        # Get a test batch\n        X_sample, y_sample = next(iter(test_loader))\n        X_sample = X_sample.to(device)\n        \n        # Make predictions\n        _, predictions = model(X_sample)\n        \n        # Convert predictions back to original scale\n        # We need to create a dummy array with all features to use inverse_transform\n        dummy = np.zeros((predictions.shape[0], 4))\n        dummy[:, 3] = predictions.cpu().numpy().flatten()  # Put predictions in the close price column\n        \n        # Inverse transform\n        predictions_original_scale = scaler.inverse_transform(dummy)[:, 3]\n        \n        # Print first few predictions\n        print(\"\\nSample Predictions (Original Scale):\")\n        for i in range(min(5, len(predictions_original_scale))):\n            print(f\"Prediction {i+1}: {predictions_original_scale[i]:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T20:01:20.507766Z","iopub.execute_input":"2025-03-22T20:01:20.508121Z","iopub.status.idle":"2025-03-22T21:16:14.090363Z","shell.execute_reply.started":"2025-03-22T20:01:20.508093Z","shell.execute_reply":"2025-03-22T21:16:14.089415Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing existing dataframe with shape: (432946, 4)\nEpoch 1/50 - Train Loss: 0.000096, Val Loss: 0.000008\nEpoch 2/50 - Train Loss: 0.000010, Val Loss: 0.000107\nEpoch 3/50 - Train Loss: 0.000008, Val Loss: 0.000027\nEpoch 4/50 - Train Loss: 0.000007, Val Loss: 0.000018\nEpoch 5/50 - Train Loss: 0.000007, Val Loss: 0.000014\nEpoch 6/50 - Train Loss: 0.000006, Val Loss: 0.000025\nEpoch 7/50 - Train Loss: 0.000006, Val Loss: 0.000017\nEpoch 8/50 - Train Loss: 0.000004, Val Loss: 0.000008\nEpoch 9/50 - Train Loss: 0.000004, Val Loss: 0.000007\nEpoch 10/50 - Train Loss: 0.000004, Val Loss: 0.000017\nEpoch 11/50 - Train Loss: 0.000004, Val Loss: 0.000005\nEpoch 12/50 - Train Loss: 0.000004, Val Loss: 0.000005\nEpoch 13/50 - Train Loss: 0.000004, Val Loss: 0.000005\nEpoch 14/50 - Train Loss: 0.000004, Val Loss: 0.000014\nEpoch 15/50 - Train Loss: 0.000004, Val Loss: 0.000006\nEpoch 16/50 - Train Loss: 0.000004, Val Loss: 0.000019\nEpoch 17/50 - Train Loss: 0.000004, Val Loss: 0.000014\nEpoch 18/50 - Train Loss: 0.000004, Val Loss: 0.000020\nEpoch 19/50 - Train Loss: 0.000003, Val Loss: 0.000103\nEpoch 20/50 - Train Loss: 0.000003, Val Loss: 0.003056\nEpoch 21/50 - Train Loss: 0.000002, Val Loss: 0.007860\nEpoch 22/50 - Train Loss: 0.000002, Val Loss: 0.008329\nEpoch 23/50 - Train Loss: 0.000002, Val Loss: 0.010232\nEpoch 24/50 - Train Loss: 0.000002, Val Loss: 0.014151\nEpoch 25/50 - Train Loss: 0.000002, Val Loss: 0.015511\nEpoch 26/50 - Train Loss: 0.000002, Val Loss: 0.018475\nEpoch 27/50 - Train Loss: 0.000002, Val Loss: 0.019894\nEpoch 28/50 - Train Loss: 0.000002, Val Loss: 0.026795\nEpoch 29/50 - Train Loss: 0.000002, Val Loss: 0.030657\nEpoch 30/50 - Train Loss: 0.000002, Val Loss: 0.034144\nEpoch 31/50 - Train Loss: 0.000001, Val Loss: 0.036908\nEpoch 32/50 - Train Loss: 0.000001, Val Loss: 0.037948\nEpoch 33/50 - Train Loss: 0.000001, Val Loss: 0.040472\nEpoch 34/50 - Train Loss: 0.000001, Val Loss: 0.040658\nEpoch 35/50 - Train Loss: 0.000001, Val Loss: 0.041792\nEpoch 36/50 - Train Loss: 0.000001, Val Loss: 0.042845\nEpoch 37/50 - Train Loss: 0.000001, Val Loss: 0.045955\nEpoch 38/50 - Train Loss: 0.000001, Val Loss: 0.045590\nEpoch 39/50 - Train Loss: 0.000001, Val Loss: 0.047338\nEpoch 40/50 - Train Loss: 0.000001, Val Loss: 0.047558\nEpoch 41/50 - Train Loss: 0.000001, Val Loss: 0.046997\nEpoch 42/50 - Train Loss: 0.000001, Val Loss: 0.047624\nEpoch 43/50 - Train Loss: 0.000001, Val Loss: 0.047809\nEpoch 44/50 - Train Loss: 0.000001, Val Loss: 0.048109\nEpoch 45/50 - Train Loss: 0.000001, Val Loss: 0.047877\nEpoch 46/50 - Train Loss: 0.000001, Val Loss: 0.047858\nEpoch 47/50 - Train Loss: 0.000001, Val Loss: 0.048312\nEpoch 48/50 - Train Loss: 0.000001, Val Loss: 0.047565\nEpoch 49/50 - Train Loss: 0.000001, Val Loss: 0.048052\nEpoch 50/50 - Train Loss: 0.000001, Val Loss: 0.047969\nModel saved to 'multi_agent_trading_model.pth'\n\nSample Predictions (Original Scale):\nPrediction 1: 22429.25\nPrediction 2: 22432.32\nPrediction 3: 22434.13\nPrediction 4: 22433.61\nPrediction 5: 22433.16\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# import torch\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import MinMaxScaler\n\n# # Define the model classes (need to be identical to the training code)\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class SelfAttention(nn.Module):\n#     def __init__(self, hidden_dim):\n#         super(SelfAttention, self).__init__()\n#         self.query = nn.Linear(hidden_dim, hidden_dim)\n#         self.key = nn.Linear(hidden_dim, hidden_dim)\n#         self.value = nn.Linear(hidden_dim, hidden_dim)\n#         self.scale = torch.sqrt(torch.tensor(hidden_dim, dtype=torch.float))\n    \n#     def forward(self, x):\n#         Q = self.query(x)\n#         K = self.key(x)\n#         V = self.value(x)\n#         scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale\n#         attn_weights = F.softmax(scores, dim=-1)\n#         attn_output = torch.bmm(attn_weights, V)\n#         return attn_output, attn_weights\n\n# class AgentNetwork(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, lstm_layers, seq_len):\n#         super(AgentNetwork, self).__init__()\n#         self.seq_len = seq_len\n#         self.hidden_dim = hidden_dim\n#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=lstm_layers, batch_first=True)\n#         self.attention = SelfAttention(hidden_dim)\n#         self.fc = nn.Linear(seq_len * hidden_dim, hidden_dim)\n#         self.out = nn.Linear(hidden_dim, 1)\n        \n#     def forward(self, x):\n#         lstm_out, _ = self.lstm(x)\n#         attn_out, _ = self.attention(lstm_out)\n#         flat = attn_out.contiguous().view(x.size(0), -1)\n#         features = F.relu(self.fc(flat))\n#         prediction = self.out(features)\n#         return prediction, features\n\n# class FusionLayer(nn.Module):\n#     def __init__(self, num_agents, agent_feature_dim, fusion_hidden_dim):\n#         super(FusionLayer, self).__init__()\n#         self.fc1 = nn.Linear(num_agents * agent_feature_dim, fusion_hidden_dim)\n#         self.fc2 = nn.Linear(fusion_hidden_dim, 1)\n        \n#     def forward(self, features_list):\n#         fusion_input = torch.cat(features_list, dim=1)\n#         x = F.relu(self.fc1(fusion_input))\n#         output = self.fc2(x)\n#         return output\n\n# class MultiAgentEnsemble(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, lstm_layers, seq_len, num_agents, fusion_hidden_dim):\n#         super(MultiAgentEnsemble, self).__init__()\n#         self.num_agents = num_agents\n#         self.agents = nn.ModuleList([\n#             AgentNetwork(input_dim, hidden_dim, lstm_layers, seq_len) for _ in range(num_agents)\n#         ])\n#         self.fusion = FusionLayer(num_agents, hidden_dim, fusion_hidden_dim)\n        \n#     def forward(self, x):\n#         agent_predictions = []\n#         agent_features = []\n#         for agent in self.agents:\n#             pred, feat = agent(x)\n#             agent_predictions.append(pred)\n#             agent_features.append(feat)\n#         fusion_output = self.fusion(agent_features)\n#         return agent_predictions, fusion_output\n\n# # Function to predict the next price\n# def predict_next_price(model, df, seq_length=30):\n#     \"\"\"Simple function to predict the next price using the last seq_length data points\"\"\"\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     model.to(device)\n#     model.eval()\n    \n#     # Get the most recent data\n#     recent_data = df[['open', 'high', 'low', 'close']].iloc[-seq_length:].values\n    \n#     # Scale the data\n#     scaler = MinMaxScaler(feature_range=(0, 1))\n#     recent_data_scaled = scaler.fit_transform(recent_data)\n    \n#     # Convert to tensor and add batch dimension\n#     input_tensor = torch.FloatTensor(recent_data_scaled).unsqueeze(0).to(device)\n    \n#     # Make prediction\n#     with torch.no_grad():\n#         _, prediction = model(input_tensor)\n    \n#     # Create dummy array for inverse scaling\n#     dummy = np.zeros((1, 4))\n#     dummy[0, 3] = prediction.item()  # Put prediction in close column\n    \n#     # Inverse transform to get the actual price\n#     predicted_price = scaler.inverse_transform(dummy)[0, 3]\n    \n#     return predicted_price\n\n# # Main execution\n# if __name__ == \"__main__\":\n#     # Model hyperparameters (must match the trained model)\n#     input_dim = 4\n#     hidden_dim = 64\n#     lstm_layers = 1\n#     seq_len = 30\n#     num_agents = 3\n#     fusion_hidden_dim = 128\n    \n#     # Load the model\n#     model = MultiAgentEnsemble(\n#         input_dim=input_dim,\n#         hidden_dim=hidden_dim,\n#         lstm_layers=lstm_layers,\n#         seq_len=seq_len,\n#         num_agents=num_agents,\n#         fusion_hidden_dim=fusion_hidden_dim\n#     )\n    \n#     # Load model weights\n#     model.load_state_dict(torch.load('/kaggle/working/multi_agent_trading_model.pth'))\n#     print(\"Model loaded successfully.\")\n    \n#     # Predict using your DataFrame (df)\n#     predicted_price = predict_next_price(model, df, seq_len)\n    \n#     print(f\"Predicted next price: {predicted_price:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T21:43:17.693536Z","iopub.execute_input":"2025-03-22T21:43:17.693958Z","iopub.status.idle":"2025-03-22T21:43:17.742656Z","shell.execute_reply.started":"2025-03-22T21:43:17.693918Z","shell.execute_reply":"2025-03-22T21:43:17.741937Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully.\nPredicted next price: 23569.48\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Define the model classes (need to be identical to the training code)\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SelfAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Linear(hidden_dim, hidden_dim)\n        self.key = nn.Linear(hidden_dim, hidden_dim)\n        self.value = nn.Linear(hidden_dim, hidden_dim)\n        self.scale = torch.sqrt(torch.tensor(hidden_dim, dtype=torch.float))\n    \n    def forward(self, x):\n        Q = self.query(x)\n        K = self.key(x)\n        V = self.value(x)\n        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        return attn_output, attn_weights\n\nclass AgentNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, lstm_layers, seq_len):\n        super(AgentNetwork, self).__init__()\n        self.seq_len = seq_len\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=lstm_layers, batch_first=True)\n        self.attention = SelfAttention(hidden_dim)\n        self.fc = nn.Linear(seq_len * hidden_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        attn_out, _ = self.attention(lstm_out)\n        flat = attn_out.contiguous().view(x.size(0), -1)\n        features = F.relu(self.fc(flat))\n        prediction = self.out(features)\n        return prediction, features\n\nclass FusionLayer(nn.Module):\n    def __init__(self, num_agents, agent_feature_dim, fusion_hidden_dim):\n        super(FusionLayer, self).__init__()\n        self.fc1 = nn.Linear(num_agents * agent_feature_dim, fusion_hidden_dim)\n        self.fc2 = nn.Linear(fusion_hidden_dim, 1)\n        \n    def forward(self, features_list):\n        fusion_input = torch.cat(features_list, dim=1)\n        x = F.relu(self.fc1(fusion_input))\n        output = self.fc2(x)\n        return output\n\nclass MultiAgentEnsemble(nn.Module):\n    def __init__(self, input_dim, hidden_dim, lstm_layers, seq_len, num_agents, fusion_hidden_dim):\n        super(MultiAgentEnsemble, self).__init__()\n        self.num_agents = num_agents\n        self.agents = nn.ModuleList([\n            AgentNetwork(input_dim, hidden_dim, lstm_layers, seq_len) for _ in range(num_agents)\n        ])\n        self.fusion = FusionLayer(num_agents, hidden_dim, fusion_hidden_dim)\n        \n    def forward(self, x):\n        agent_predictions = []\n        agent_features = []\n        for agent in self.agents:\n            pred, feat = agent(x)\n            agent_predictions.append(pred)\n            agent_features.append(feat)\n        fusion_output = self.fusion(agent_features)\n        return agent_predictions, fusion_output\n\n# Function to predict the next price\ndef predict_next_price(model, df, scaler, seq_length=30):\n    \"\"\"\n    Predict the next price using the trained model and the existing scaler\n    \n    Args:\n        model: Trained model\n        df: DataFrame with OHLC data\n        scaler: The SAME scaler used during training\n        seq_length: Length of input sequence\n        \n    Returns:\n        predicted_price: The predicted next price\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    \n    # Get the most recent data\n    recent_data = df[['open', 'high', 'low', 'close']].iloc[-seq_length:].values\n    \n    # Use the same scaler that was used during training\n    recent_data_scaled = scaler.transform(recent_data)\n    \n    # Convert to tensor and add batch dimension\n    input_tensor = torch.FloatTensor(recent_data_scaled).unsqueeze(0).to(device)\n    \n    # Make prediction\n    with torch.no_grad():\n        _, prediction = model(input_tensor)\n    \n    # Create dummy array for inverse scaling\n    dummy = np.zeros((1, 4))\n    dummy[0, 3] = prediction.item()  # Put prediction in close column\n    \n    # Inverse transform to get the actual price\n    predicted_price = scaler.inverse_transform(dummy)[0, 3]\n    \n    return predicted_price\n\n# Function to load or create a scaler\ndef get_scaler(df):\n    \"\"\"Create and fit a scaler on the entire dataset, as would have been done during training\"\"\"\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler.fit(df[['open', 'high', 'low', 'close']].values)\n    return scaler\n\n# Main execution\nif __name__ == \"__main__\":\n    # Model hyperparameters (must match the trained model)\n    input_dim = 4\n    hidden_dim = 64\n    lstm_layers = 1\n    seq_len = 30\n    num_agents = 3\n    fusion_hidden_dim = 128\n    \n    # Load the model\n    model = MultiAgentEnsemble(\n        input_dim=input_dim,\n        hidden_dim=hidden_dim,\n        lstm_layers=lstm_layers,\n        seq_len=seq_len,\n        num_agents=num_agents,\n        fusion_hidden_dim=fusion_hidden_dim\n    )\n    \n    # Load model weights\n    model.load_state_dict(torch.load('/kaggle/working/multi_agent_trading_model.pth'))\n    print(\"Model loaded successfully.\")\n    \n    # Get or create the scaler using the entire dataset\n    # This is important because we need to use the same scaling as during training\n    scaler = get_scaler(df)\n    \n    # Predict using your DataFrame (df)\n    predicted_price = predict_next_price(model, df, scaler, seq_len)\n    \n    print(f\"Predicted next price: {predicted_price:.2f}\")\n    \n    # You can also get the current price for comparison\n    current_price = df['close'].iloc[-1]\n    print(f\"Current price: {current_price:.2f}\")\n    print(f\"Predicted change: {predicted_price - current_price:.2f} ({((predicted_price/current_price)-1)*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T21:46:32.324386Z","iopub.execute_input":"2025-03-22T21:46:32.324697Z","iopub.status.idle":"2025-03-22T21:46:32.375432Z","shell.execute_reply.started":"2025-03-22T21:46:32.324674Z","shell.execute_reply":"2025-03-22T21:46:32.374698Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully.\nPredicted next price: 26728.63\nCurrent price: 23563.15\nPredicted change: 3165.48 (13.43%)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"(23563/26728)*100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T21:48:41.526132Z","iopub.execute_input":"2025-03-22T21:48:41.526483Z","iopub.status.idle":"2025-03-22T21:48:41.532776Z","shell.execute_reply.started":"2025-03-22T21:48:41.526458Z","shell.execute_reply":"2025-03-22T21:48:41.532032Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"88.1584854833882"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}